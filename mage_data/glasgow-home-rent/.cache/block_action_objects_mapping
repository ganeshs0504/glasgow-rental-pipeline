{"block_file": {"charts/completed_pipeline_runs_daily_home_rent_etl_pipeline_k0.py:chart:python:completed pipeline runs daily home rent etl pipeline k0": {"content": "\n@data_source\ndef d(df):\n    return df[df['status'] == 'completed']\n", "file_path": "charts/completed_pipeline_runs_daily_home_rent_etl_pipeline_k0.py", "language": "python", "type": "chart", "uuid": "completed_pipeline_runs_daily_home_rent_etl_pipeline_k0"}, "charts/failed_pipeline_runs_daily_home_rent_etl_pipeline_g7.py:chart:python:failed pipeline runs daily home rent etl pipeline g7": {"content": "\n@data_source\ndef d(df):\n    return df[df['status'] == 'failed']\n", "file_path": "charts/failed_pipeline_runs_daily_home_rent_etl_pipeline_g7.py", "language": "python", "type": "chart", "uuid": "failed_pipeline_runs_daily_home_rent_etl_pipeline_g7"}, "custom/df_inter_test_delete.py:custom:python:df inter test delete": {"content": "if 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@custom\ndef transform_custom(df, *args, **kwargs):\n    \"\"\"\n    args: The output from any upstream parent blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your custom logic here\n    print(df[df['origin_lat']==0]['station_1'].unique())\n\n    return df\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert len(df[df['origin_lat']==0]['station_1'].unique()) == 1, 'Error finding some stations'\n", "file_path": "custom/df_inter_test_delete.py", "language": "python", "type": "custom", "uuid": "df_inter_test_delete"}, "custom/rent_scraper_script.py:custom:python:rent scraper script": {"content": "import requests\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nimport pandas as pd\nimport time\nfrom tqdm.auto import tqdm\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\nif 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nxpaths = {\n    'cookies_reject_btn': \"//button[@id='onetrust-reject-all-handler']\",\n    'rent_listing': \"//div[@class='l-searchResult is-list']\",\n    'address': \".//address\",\n    'price': \".//*[@class='propertyCard-priceValue']\",\n    'beds': \".//*[@class='property-information']/span[contains(@class, 'bed')]/following-sibling::span[1]\",\n    'bath': \".//*[@class='property-information']/span[contains(@class, 'bath')]/following-sibling::span[1]\",\n    'link': \".//div[@class='propertyCard-details']/a\",\n    'next_page': \"//div[not(contains(@class, 'disabled'))]/button[@title='Next page']\",\n    'date_added': \"//div[contains(text(), 'Added') or contains(text(), 'Reduced')]\",\n    'let_available_date': \"//*[contains(text(), 'Let available date: ')]/..//dd\",\n    'deposit': \"//*[contains(text(), 'Deposit')]/../dd\",\n    'furnish_type': \"//*[contains(text(), 'Furnish type')]/..//dd\",\n    'property_type': \"//*[@data-testid='svg-house']/../../span\",\n    'station_list': \"//div[@id='Stations-panel']//li\"\n}\n\ndef fetch_listings(driver):\n    data = []\n    wait = WebDriverWait(driver, 5)\n    wait.until(EC.visibility_of_element_located((By.XPATH, xpaths['rent_listing'])))\n    item_list = driver.find_elements(By.XPATH, xpaths['rent_listing'])\n    for ele in item_list:\n        # webdriver.ActionChains(driver).move_to_element(ele).perform()\n        link = ele.find_element(By.XPATH, xpaths['link']).get_attribute('href')\n        address = ele.find_element(By.XPATH, xpaths['address']).text\n        price = ele.find_element(By.XPATH, xpaths['price']).text\n        try:\n            beds = ele.find_element(By.XPATH, xpaths['beds']).text\n        except Exception:\n            beds = None\n        try:\n            bath = ele.find_element(By.XPATH, xpaths['bath']).text\n        except Exception:\n            bath = None\n    \n        data.append({\n            'link': link,\n            'address': address,\n            'price': price,\n            'beds': beds,\n            'bath': bath\n        })\n    return data\n\ndef get_texts(driver, xpath):\n    elements = driver.find_elements(By.XPATH, xpath)\n    values = []\n    for ele in elements:\n        values.append(ele.text)\n    return values\n\ndef scrape_listing_info(driver, link):\n    driver.get(link)\n    \n    date_added = driver.find_element(By.XPATH, xpaths['date_added']).text.split()[-1]\n    available_date = driver.find_element(By.XPATH, xpaths['let_available_date']).text\n    deposit = driver.find_element(By.XPATH, xpaths['deposit']).text\n    furnish_type = driver.find_element(By.XPATH, xpaths['furnish_type']).text\n    try:\n        property_type = driver.find_element(By.XPATH, xpaths['property_type']).text\n    except Exception:\n        property_type = None\n\n    station_list = get_texts(driver, xpaths['station_list'])\n    station_1 = station_list[0].split(\"\\n\")[0] if len(station_list) > 0 else None\n    station_2 = station_list[1].split(\"\\n\")[0] if len(station_list) > 1 else None\n    station_3 = station_list[2].split(\"\\n\")[0] if len(station_list) > 2 else None\n    station_1_dist = station_list[0].split(\"\\n\")[-1] if len(station_list) > 0 else None\n    station_2_dist = station_list[1].split(\"\\n\")[-1] if len(station_list) > 1 else None\n    station_3_dist = station_list[2].split(\"\\n\")[-1] if len(station_list) > 2 else None\n\n    return ({\n        'date_added': date_added,\n        'available_date': available_date,\n        'deposit': deposit,\n        'furnish_type': furnish_type,\n        'property_type': property_type,\n        'station_1': station_1,\n        'station_1_dist': station_1_dist,\n        'station_2': station_2,\n        'station_2_dist': station_2_dist,\n        'station_3': station_3,\n        'station_3_dist': station_3_dist,\n    })\n\n\n@custom\ndef transform_custom(*args, **kwargs):\n    chrome_options = webdriver.ChromeOptions()\n    chrome_options.add_argument('--no-sandbox')\n    chrome_options.add_argument('--window-size=1920,1080')\n    chrome_options.add_argument('--ignore-ssl-errors=yes')\n    chrome_options.add_argument('--ignore-certificate-errors')\n    chrome_options.add_argument('--headless')\n    chrome_options.add_argument('--disable-gpu')\n    driver = webdriver.Remote(command_executor='host.docker.internal:4444/wd/hub', options=chrome_options)\n\n    url = 'https://www.rightmove.co.uk/property-to-rent/find.html?locationIdentifier=REGION%5E550&propertyTypes=&includeLetAgreed=false&mustHave=&dontShow=&furnishTypes=&keywords='\n\n    driver.get(url)\n\n    driver.find_element(By.XPATH, xpaths['cookies_reject_btn']).click()\n\n    final_structured_listings = []\n    while True:\n        structured_listings = fetch_listings(driver)\n        element = driver.find_element(By.XPATH, \"//a[@title='Contact us']\")\n        webdriver.ActionChains(driver).move_to_element(element).perform()\n        final_structured_listings.extend(structured_listings)\n        try:\n            wait = WebDriverWait(driver, 5)\n            wait.until(EC.visibility_of_element_located((By.XPATH, xpaths['next_page'])))\n            driver.find_element(By.XPATH, xpaths['next_page']).click()\n        except Exception as e:\n            print(e)\n            print(\"Next button disabled: End of page\")\n            print(\"Exiting Script\")\n            break\n        # time.sleep(2)\n\n    df = pd.DataFrame(final_structured_listings)\n\n    date_added = []\n    available_date = []\n    deposit = []\n    furnish_type = []\n    property_type = []\n    station_1 = []\n    station_1_dist = []\n    station_2 = []\n    station_2_dist = []\n    station_3 = []\n    station_3_dist = []\n\n    for i, row in df.iterrows():\n        scraped_data = scrape_listing_info(driver, row['link'])\n        \n        date_added.append(scraped_data['date_added'])\n        available_date.append(scraped_data['available_date'])\n        deposit.append(scraped_data['deposit'])\n        furnish_type.append(scraped_data['furnish_type'])\n        property_type.append(scraped_data['property_type'])\n        station_1.append(scraped_data['station_1'])\n        station_1_dist.append(scraped_data['station_1_dist'])\n        station_2.append(scraped_data['station_2'])\n        station_2_dist.append(scraped_data['station_2_dist'])\n        station_3.append(scraped_data['station_3'])\n        station_3_dist.append(scraped_data['station_3_dist'])\n\n\n    driver.quit()\n\n    df['date_added'] = date_added\n    df['available_date'] = available_date\n    df['deposit'] = deposit\n    df['furnish_type'] = furnish_type\n    df['property_type'] = property_type\n    df['station_1'] = station_1\n    df['station_1_dist'] = station_1_dist\n    df['station_2'] = station_2\n    df['station_2_dist'] = station_2_dist\n    df['station_3'] = station_3\n    df['station_3_dist'] = station_3_dist\n\n    return df\n\n\n@test\ndef test_output(output, *args):\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, \"There is no output\"\n    # df = output['data']\n    # assert len(output['date_added']) == df.shape[0], f\"expected{df.shape[0]}; got{len(output['date_added'])}\"\n    # assert len(output['available_date']) == df.shape[0], f\"expected{df.shape[0]}; got{len(output['available_date'])}\"\n    # assert len(output['deposit']) == df.shape[0], f\"expected{df.shape[0]}; got{len(output['deposit'])}\"\n    # assert len(output['furnish_type']) == df.shape[0], f\"expected{df.shape[0]}; got{len(output['furnish_type'])}\"\n    # assert len(output['property_type']) == df.shape[0], f\"expected{df.shape[0]}; got{len(output['property_type'])}\"\n    # assert len(output['station_1']) == df.shape[0], f\"expected{df.shape[0]}; got{len(output['station_1'])}\"\n    # assert len(output['station_1_dist']) == df.shape[0], f\"expected{df.shape[0]}; got{len(output['station_1_dist'])}\"\n    # assert len(output['station_2']) == df.shape[0], f\"expected{df.shape[0]}; got{len(output['station_2'])}\"\n    # assert len(output['station_2_dist']) == df.shape[0], f\"expected{df.shape[0]}; got{len(output['station_2_dist'])}\"\n    # assert len(output['station_3']) == df.shape[0], f\"expected{df.shape[0]}; got{len(output['station_3'])}\"\n    # assert len(output['station_3_dist']) == df.shape[0], f\"expected{df.shape[0]}; got{len(output['station_3_dist'])}\"\n", "file_path": "custom/rent_scraper_script.py", "language": "python", "type": "custom", "uuid": "rent_scraper_script"}, "custom/scraped_data_to_csv.py:custom:python:scraped data to csv": {"content": "if 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@custom\ndef transform_custom(*args, **kwargs):\n    # df = output['data']\n\n    # df['date_added'] = output['date_added']\n    # df['available_date'] = output['available_date']\n    # df['deposit'] = output['deposit']\n    # df['furnish_type'] = output['furnish_type']\n    # df['property_type'] = output['property_type']\n    # df['station_1'] = output['station_1']\n    # df['station_1_dist'] = output['station_1_dist']\n    # df['station_2'] = output['station_2']\n    # df['station_2_dist'] = output['station_2_dist']\n    # df['station_3'] = output['station_3']\n    # df['station_3_dist'] = output['station_3_dist']\n\n    # df.to_csv('output.csv', index=False, encoding='utf-8')\n\n    return \"Successfully written\"\n", "file_path": "custom/scraped_data_to_csv.py", "language": "python", "type": "custom", "uuid": "scraped_data_to_csv"}, "data_exporters/custom_data_exporter.py:data_exporter:python:custom data exporter": {"content": "from mage_ai.io.file import FileIO\nfrom pandas import DataFrame\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_file(df, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to filesystem.\n\n    Docs: https://docs.mage.ai/design/data-loading#fileio\n    \"\"\"\n    df.to_csv('output.csv', index=False, encoding='utf-8')\n", "file_path": "data_exporters/custom_data_exporter.py", "language": "python", "type": "data_exporter", "uuid": "custom_data_exporter"}, "data_exporters/export_titanic_clean.py:data_exporter:python:export titanic clean": {"content": "from mage_ai.io.file import FileIO\nfrom pandas import DataFrame\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_file(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to filesystem.\n\n    Docs: https://docs.mage.ai/design/data-loading#example-loading-data-from-a-file\n    \"\"\"\n    filepath = 'titanic_clean.csv'\n    FileIO().export(df, filepath)\n", "file_path": "data_exporters/export_titanic_clean.py", "language": "python", "type": "data_exporter", "uuid": "export_titanic_clean"}, "data_exporters/rentflow_export_bigquery.py:data_exporter:python:rentflow export bigquery": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.bigquery import BigQuery\nfrom mage_ai.io.config import ConfigFileLoader\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_big_query(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a BigQuery warehouse.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#bigquery\n    \"\"\"\n    table_id = 'data-eng-zoomcamp-426514.rentflow_glasgow.listings'\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    BigQuery.with_config(ConfigFileLoader(config_path, config_profile)).export(\n        df,\n        table_id,\n        if_exists='replace',  # Specify resolution policy if table name already exists\n    )\n", "file_path": "data_exporters/rentflow_export_bigquery.py", "language": "python", "type": "data_exporter", "uuid": "rentflow_export_bigquery"}, "data_exporters/rentflow_export_postgres.py:data_exporter:python:rentflow export postgres": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_postgres(output, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a PostgreSQL database.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#postgresql\n    \"\"\"\n    print(output.shape)\n    schema_name = 'postgres'  # Specify the name of the schema to export data to\n    table_name = 'listings'  # Specify the name of the table to export data to\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'dev'\n\n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        loader.export(\n            output,\n            schema_name,\n            table_name,\n            index=False,  # Specifies whether to include index in exported table\n            if_exists='replace',  # Specify resolution policy if table name already exists\n        )\n", "file_path": "data_exporters/rentflow_export_postgres.py", "language": "python", "type": "data_exporter", "uuid": "rentflow_export_postgres"}, "data_exporters/test.py:data_exporter:python:test": {"content": "from pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_big_query(df: DataFrame, **kwargs) -> None:\n    print(df.info())\n", "file_path": "data_exporters/test.py", "language": "python", "type": "data_exporter", "uuid": "test"}, "data_loaders/ingest_data_from_csv.py:data_loader:python:ingest data from csv": {"content": "from mage_ai.io.file import FileIO\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_file(*args, **kwargs):\n    \"\"\"\n    Template for loading data from filesystem.\n    Load data from 1 file or multiple file directories.\n\n    For multiple directories, use the following:\n        FileIO().load(file_directories=['dir_1', 'dir_2'])\n\n    Docs: https://docs.mage.ai/design/data-loading#fileio\n    \"\"\"\n    filepath = 'output.csv'\n\n    return FileIO().load(filepath)\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/ingest_data_from_csv.py", "language": "python", "type": "data_loader", "uuid": "ingest_data_from_csv"}, "data_loaders/load_titanic.py:data_loader:python:load titanic": {"content": "import io\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(**kwargs) -> DataFrame:\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv?raw=True'\n\n    return pd.read_csv(url)\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_titanic.py", "language": "python", "type": "data_loader", "uuid": "load_titanic"}, "transformers/fill_in_missing_values.py:transformer:python:fill in missing values": {"content": "from pandas import DataFrame\nimport math\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef select_number_columns(df: DataFrame) -> DataFrame:\n    return df[['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Survived']]\n\n\ndef fill_missing_values_with_median(df: DataFrame) -> DataFrame:\n    for col in df.columns:\n        values = sorted(df[col].dropna().tolist())\n        median_value = values[math.floor(len(values) / 2)]\n        df[[col]] = df[[col]].fillna(median_value)\n    return df\n\n\n@transformer\ndef transform_df(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        df (DataFrame): Data frame from parent block.\n\n    Returns:\n        DataFrame: Transformed data frame\n    \"\"\"\n    # Specify your transformation logic here\n\n    return fill_missing_values_with_median(select_number_columns(df))\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "transformers/fill_in_missing_values.py", "language": "python", "type": "transformer", "uuid": "fill_in_missing_values"}, "transformers/rentflow_gmap_data_integration.py:transformer:python:rentflow gmap data integration": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader, ConfigKey, EnvironmentVariableLoader\nimport pandas as pd\nfrom os import path\nimport googlemaps\nfrom datetime import datetime, date\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n\ndef get_gmaps_direction(gmaps, origin, dest='Glasgow Central', dep_time=datetime(date.today().year, date.today().month, date.today().day, 9, 0, 0), mode='transit'):\n    response = gmaps.directions(\n        origin = origin,\n        destination = dest,\n        mode = mode,\n        transit_mode = 'rail',\n        region = 'uk',\n        departure_time = dep_time,\n        units = 'metric'\n    )\n    return response \n\n\n@transformer\ndef transform(df, *args, **kwargs):\n\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'gmap_dev'\n    config = ConfigFileLoader(config_path, config_profile)\n    API_KEY = config.get('API_KEY')\n\n    gmaps = googlemaps.Client(key=API_KEY)\n\n    train_journey_time_ = []\n    number_of_trains_ = []\n    total_number_of_stops_ = []\n    walking_time_ = []\n    walking_distance_ = []\n    origin_lat_ = []\n    origin_lng_ = []\n    total_time_ = []\n    total_dist_ = []\n\n    for i, row in df.iterrows():\n        try:\n            train_journey_time = 0\n            number_of_trains = 0\n            total_number_of_stops = 0\n            walking_time = 0\n            walking_distance = 0\n            origin_lng = 0\n            origin_lat = 0\n            total_time = 0\n            total_dist = 0\n        \n            response = get_gmaps_direction(gmaps, row['station_1']+', Glasgow')\n                \n            legs = response[0]['legs'][0]\n        \n            origin_lat = legs['start_location']['lat']\n            origin_lng = legs['start_location']['lng']\n        \n            total_time = legs['duration']['value'] / 60\n            total_dist = legs['distance']['value']\n        \n            for step in legs['steps']:\n                if step['travel_mode'] == 'TRANSIT':\n                    number_of_trains += 1\n                    total_number_of_stops += step['transit_details']['num_stops']\n                    train_journey_time += step['duration']['value']\n                elif step['travel_mode'] == 'WALKING':\n                    walking_distance += step['distance']['value']\n                    walking_time += step['duration']['value'] / 60\n        except Exception as e:\n            print(e, row['station_1'])\n            # train_journey_time = 0\n            # number_of_trains = 0\n            # total_number_of_stops = 0\n            # walking_time = 0\n            # walking_distance = 0\n            # origin_lat = 0\n            # origin_lng = 0\n            # total_time = 0\n            # total_dist = 0\n        \n        train_journey_time_.append(train_journey_time)\n        number_of_trains_.append(number_of_trains)\n        total_number_of_stops_.append(total_number_of_stops)\n        walking_time_.append(walking_time)\n        walking_distance_.append(walking_distance)\n        origin_lat_.append(origin_lat)\n        origin_lng_.append(origin_lng)\n        total_time_.append(total_time)\n        total_dist_.append(total_dist)\n\n    df['train_journey_time'] = train_journey_time_\n    df['number_of_trains'] = number_of_trains_\n    df['total_number_of_stops'] = total_number_of_stops_\n    df['walking_time'] = walking_time_\n    df['walking_distance'] = walking_distance_\n    df['origin_lat'] = origin_lat_\n    df['origin_lng'] = origin_lng_\n    df['total_time'] = total_time_\n    df['total_dist'] = total_dist_\n    \n\n\n    return df\n\n\n@test\ndef test_output(output, *args) -> None:\n    assert len(output[output['origin_lat']==0]['station_1'].unique()) == 1, 'Error finding some stations'\n", "file_path": "transformers/rentflow_gmap_data_integration.py", "language": "python", "type": "transformer", "uuid": "rentflow_gmap_data_integration"}, "transformers/rentflo_data_transformer.py:transformer:python:rentflo data transformer": {"content": "import pandas as pd\nfrom datetime import datetime, timedelta\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef convert_pricing(str):\n    if pd.isna(str) or (str is None):\n        return None\n    return float(str.split()[0].replace(',', '').replace('\u00a3', ''))\n\ndef convert_distance(str):\n    if pd.isna(str) or (str is None):\n        return None\n    return float(str.split()[0])\n\ndef convert_datetime(str):\n    if pd.isna(str) or (str is None):\n        return pd.NaT\n    str = str.lower()\n    if str == 'now' or str == 'today':\n        return datetime.today().date()\n    elif str == 'yesterday':\n        return (datetime.today() - timedelta(days=1)).date()\n    else:\n        return pd.to_datetime(str, format='%d/%m/%Y').date()\n\n\n@transformer\ndef transform(df, *args, **kwargs):\n    \n    df = df.drop_duplicates(ignore_index=True)\n\n    df['price'] = df['price'].apply(convert_pricing)\n\n    df['deposit'] = df['deposit'].replace('Ask agent', None)\n    df['deposit'] = df['deposit'].apply(convert_pricing)\n\n    df['station_1_dist'] = df['station_1_dist'].apply(convert_distance)\n    df['station_2_dist'] = df['station_2_dist'].apply(convert_distance)\n    df['station_3_dist'] = df['station_3_dist'].apply(convert_distance)\n\n    df['furnish_type'] = df['furnish_type'].replace('Ask agent', None).replace('Now', None)\n\n    df['date_added'] = df['date_added'].replace('Ask agent', None)\n    df['available_date'] = df['available_date'].replace('Ask agent', None)\n\n    df['date_added'] = df['date_added'].apply(convert_datetime)\n    df['available_date'] = df['available_date'].apply(convert_datetime)\n    df['date_added'] = pd.to_datetime(df['date_added'])\n    df['available_date'] = pd.to_datetime(df['available_date'])\n\n    df['beds'] = df['beds'].fillna(0.0).astype(int)\n    df['bath'] = df['bath'].fillna(0.0).astype(int)\n\n    return df\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "transformers/rentflo_data_transformer.py", "language": "python", "type": "transformer", "uuid": "rentflo_data_transformer"}}, "custom_block_template": {}, "mage_template": {"data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}